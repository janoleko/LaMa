---
output: github_document
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  fig.path = "man/figures/README-",
  out.width = "75%",
  fig.align = "center"
)
```

# {LaMa}: Latent Markov model likelihood evaluation in C++ <img src="man/figures/Logo_LaMa.png" align="right" height=170>

<!-- badges: start -->
[![CRAN status](https://www.r-pkg.org/badges/version/LaMa)](https://CRAN.R-project.org/package=LaMa)
[![metacran downloads](https://cranlogs.r-pkg.org/badges/last-month/LaMa)](https://cran.r-project.org/package=LaMa)
[![R-CMD-check](https://github.com/janoleko/LaMa/actions/workflows/R-CMD-check.yaml/badge.svg)](https://github.com/janoleko/LaMa/actions/workflows/R-CMD-check.yaml)
<!-- badges: end -->

A plethora of latent Markov models, including **hidden Markov models** (HMMs), **hidden semi-Markov models** (HSMMs), **state space models** (SSMs) as well as **continuous-time HMMs**, **continuous-time SSMs**, and **Markov-modulated marked Poisson processes** (MMMPPs) can be formulated and estimated within the same framework via directly maximizing the (approximate) likelihood using the so-called **forward algorithm** (for details see
<a href="https://teuder.github.io/rcpp4everyone_en/020_install.html" target="_blank">Zucchini et al. 2016</a>).
While these models are immensely popular for conducting inference on time series driven by latent processes, due to their great flexibility, researchers using these models in applied work often need to build highly customized models for which standard software implementation is lacking, or the construction of such models in said software is as complicated as writing fully tailored **R** code. The latter provides great flexibility and control, but suffers from slow estimation speeds that make custom solutions inconvenient. This **R** package addresses the above issues in two ways. Standard blocks of code common to all these model classes, most importantly the forward algorithm, are implemented as simple-to-use functions. These can be added like Lego blocks to an otherwise fully custom likelihood function, making building fully custom models much easier. Moreover, under the hood, these functions are written in **C++**, allowing for 10-20 times faster evaluation time, and thus drastically speeding up estimation by numerical optimizers like `nlm()` or `optim()`.

Current implementations of the forward algorithm are:

* `forward()` for models with **homogeneous** transition probabilities,
* `forward_g()` for general (pre-calculated) **inhomogeneous** transition probabilities (including **continuous-time** HMMs and points processes), and
* `forward_s()` for fitting **HSMMs**.

The functions are built to be included in the **negative log-likelihood function**, after parameters have been transformed and the *allprobs* matrix (containing all state-dependent probabilities) has been calculated. 

To serve as a powerful toolbox, this package also includes many auxiliary functions like

* the `tpm` family with for calculating transition probability matrices,
  <!-- + `tpm()` for calculating a homogeneous transition probability matrix via the multinomial logistic link,  -->
  <!-- + `tpm_g()` for calculating general inhomogeneous transition probabilty matrices,  -->
  <!-- + `tpm_p()` for calculating transition matrices of periodically inhomogeneous HMMs, -->
  <!-- + `tpm_cont()` for calculating the transition probabilites of a continuous-time Markov chain, -->
  <!-- + `tpm_hsmm()` for calculating the transition matrix of an HSMM-approximating HMM, -->

* the `stationary` family to compute stationary and periodically stationary distributions

* as well as the `stateprobs` and `viterbi` family for local and global decoding.

## Installation

You can install the released package version from [CRAN](https://CRAN.R-project.org) with:

```{r installation_cran, eval = FALSE}
install.packages("LaMa")
```

or the development version from Github:
```{r installation_github, eval = FALSE, warning=FALSE, message = FALSE}
# install.packages("devtools")
devtools::install_github("janoleko/LaMa", build_vignettes = TRUE)
```
<!-- (To install from Github, you need a functional <a href="https://teuder.github.io/rcpp4everyone_en/020_install.html" target="_blank">C++ compiler</a>.) -->

## Package documentation

To aid in building fully custom likelihood functions, this package contains several vignettes that demonstrate how to simulate data from and estimate a wide range of models using the functions included in this package:

* [Introduction to LaMa](https://janoleko.github.io/files/vignettes/LaMa/Intro_to_LaMa.pdf)
* [Inhomogeneous HMMs with covariate effects](https://janoleko.github.io/files/vignettes/LaMa/Inhomogeneous_HMMs.pdf)
* [Longitudinal data](https://janoleko.github.io/files/vignettes/LaMa/Longitudinal_data.pdf)
* [Periodic HMMs](https://janoleko.github.io/files/vignettes/LaMa/Periodic_HMM.pdf)
* [State space models](https://janoleko.github.io/files/vignettes/LaMa/State_space_models.pdf)
* [Continuous-time HMMs](https://janoleko.github.io/files/vignettes/LaMa/Continuous_time_HMMs.pdf)
* [Hidden semi-Markov models](https://janoleko.github.io/files/vignettes/LaMa/HSMMs.pdf)
* [Markov-modulated (marked) Poisson processes](https://janoleko.github.io/files/vignettes/LaMa/MMMPPs.pdf)


## Citation

When using LaMa, please cite the package as follows:
```{r citation}
citation(package = "LaMa")
```


## Example: Homogeneous HMM

#### Loading the package
```{r package}
library(LaMa)
```


#### Generating data from a 2-state HMM
Here we can use `stationary()` to compute the stationary distribution.
```{r data}
# parameters
mu = c(0, 6)
sigma = c(2, 4)
Gamma = matrix(c(0.95, 0.05, 0.15, 0.85), nrow = 2, byrow = TRUE)
delta = stationary(Gamma) # stationary HMM

# simulation
n = 10000 # rather large
set.seed(123)
s = x = rep(NA, n)
s[1] = sample(1:2, 1, prob = delta)
x[1] = rnorm(1, mu[s[1]], sigma[s[1]])
for(t in 2:n){
  s[t] = sample(1:2, 1, prob = Gamma[s[t-1],])
  x[t] = rnorm(1, mu[s[t]], sigma[s[t]])
}

plot(x[1:200], bty = "n", pch = 20, ylab = "x", 
     col = c("orange","deepskyblue")[s[1:200]])
```

#### Writing the negative log-likelihood function
Here, we build the transition probability matrix using the `tpm()` function, compute the stationary distribution using `stationary()` and calculate the log-likelihood using `forward()` in the last line.
```{r mllk}
mllk = function(theta.star, x){
  # parameter transformations for unconstraint optimization
  Gamma = tpm(theta.star[1:2])
  delta = stationary(Gamma) # stationary HMM
  mu = theta.star[3:4]
  sigma = exp(theta.star[5:6])
  # calculate all state-dependent probabilities
  allprobs = matrix(1, length(x), 2)
  for(j in 1:2){ allprobs[,j] = stats::dnorm(x, mu[j], sigma[j]) }
  # return negative for minimization
  -forward(delta, Gamma, allprobs)
}
```

#### Fitting an HMM to the data
```{r model, warning=FALSE}
theta.star = c(-1,-1,1,4,log(1),log(3)) 
# initial transformed parameters: not chosen too well
s = Sys.time()
mod = nlm(mllk, theta.star, x = x)
Sys.time()-s
```
Really fast for 10.000 data points!

#### Visualizing results
Again, we use `tpm()` and `stationary()` to tranform the unconstraint parameters to working parameters.
```{r visualization}
# transform parameters to working
Gamma = tpm(mod$estimate[1:2])
delta = stationary(Gamma) # stationary HMM
mu = mod$estimate[3:4]
sigma = exp(mod$estimate[5:6])

hist(x, prob = TRUE, bor = "white", breaks = 40, main = "")
curve(delta[1]*dnorm(x, mu[1], sigma[1]), add = TRUE, lwd = 2, col = "orange", n=500)
curve(delta[2]*dnorm(x, mu[2], sigma[2]), add = TRUE, lwd = 2, col = "deepskyblue", n=500)
curve(delta[1]*dnorm(x, mu[1], sigma[1])+delta[2]*dnorm(x, mu[2], sigma[2]),
      add = TRUE, lwd = 2, lty = "dashed", n=500)
legend("topright", col = c("orange", "deepskyblue", "black"), lwd = 2, bty = "n",
       lty = c(1,1,2), legend = c("state 1", "state 2", "marginal"))
```