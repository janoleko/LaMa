---
title: "LaMa and RTMB"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{LaMa_and_RTMB}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  out.width = "85%",
  fig.align = "center"
)
```

The rather new `R` package `RTMB` conveniently allows for automatic differentiation for non-standard statistical models written in plain `R` code, enabling the estimation very complicated models, potentially with complex random effect structures -- which is basically magic because you have access to analytic gradients without doing any calculations!

Automatic differentiation is hugely important as it allows for much higher accuracy and enormeous speed increases in the estimation of rich statistical models with potentially many parameters.

`LaMa` also provides automatic differentiation for most of its functions when the user is setting `ad = TRUE`, hence making the estimation of latent Markov models extremly convenient and fast, while making the process of model specification very smooth and less prone to errors (which at the current state tend to happen when not experienced with `RTMB`).

Here we demonstrate how to use `LaMa` and `RTMB` to fit hidden Markov models and their extensions. We always start by loading both packages.

```{r setup}
library(LaMa)
library(RTMB)
```

For the purpose of this vignette, we will analyze the `elephant` data set contained in the `LaMa` package:

```{r data}
head(elephant, 5)
```

The workflow with `RTMB` is basically always the same. We need to define the negative log-likelihood function, create an automatically differentiable objective function from it and fit the model by numerical minimization of the latter. We start by fitting a super simple stationary HMM with state-dependent gamma distributions for the step lengths and von Mises distributions for the turning angles. As a first step, we define the initial parameter list `par` and a `dat` list that contains everything else.

```{r parameters}
par = list(logmu = log(c(0.3, 1)), 
           logsigma = log(c(0.2, 0.7)),
           logkappa = log(c(0.2, 0.7)),
           eta = rep(-2, 2))
dat = list(step = elephant$step, angle = elephant$angle, N = 2)
```

We see that `par` is a named list which is really useful for accessing the parameters later on and much more convenient than indexing. I use the `dat` list to package all the data and other hyperparameters that are not to be estimated, but this is just my choice out of convenience.

We can now define the negative log-likelihood function.

```{r mllk}
mllk = function(par) {
  getAll(par, dat) # makes everything contained available without $
  Gamma = tpm(eta) # computes transition probability matrix from unconstrained eta
  delta = stationary(Gamma) # computes stationary distribution
  # exponentiating because all parameters strictly positive
  mu = exp(logmu)
  sigma = exp(logsigma)
  kappa = exp(logkappa)
  # reporting statements for later use
  REPORT(mu)
  REPORT(sigma)
  REPORT(kappa)
  # calculating all state-dependent densities
  allprobs = matrix(1, nrow = length(step), ncol = N)
  ind = which(!is.na(step) & !is.na(angle)) # only for non-NA obs.
  for(j in 1:N){
    allprobs[ind,j] = dgamma2(step[ind],mu[j],sigma[j])*dvm(angle[ind],0,kappa[j])
  }
  -forward(delta, Gamma, allprobs, ad = TRUE) # simple forward algorithm
}
```

There are a few points to note here:

-   Most prominently, the negative log-likelihood is a function of the parameters to be estimated *only* while data and other parameters are not passed as an argument at this stage. This is something to get used to, but just the way `RTMB` works.
-   The `getAll()` function is insanely useful and you should basically always use it in the first line, as it unpacks both the `par` and the `dat` list.
-   `par` and `dat` should be named lists which is also super useful as you can just access the parameters by their names after unpacking them with `getAll()` -- the names `par` and `dat` of course being arbitrary.
-   Parameter transformations are still necessary, i.e. all parameters in `par` should be unconstrained.
-   Simple `LaMa` functions work for both automatic differentiation and standard optimization. For the more complicated functions like `forward()`, currently you need to specify `ad = TRUE` to get automatic differentiation. When you are not sure if you need it, check the documentation.
-   You might wonder how on earth `RTMB` can calculate the gradient of standard distributions like the gamma or von Mises distribution. The answer is it doesn't but provides its own version of all standard distributions like `dnorm()`, `dbinom()`, etc. In this case both `dgamma2()` and `dvm()` come from `LaMa` as these are non-standard, but under the hood build on `RTMB` functions. `dgamma2()` is just a convenience function as it reparametrizes the gamma distribution in terms of mean and standard deviation, which is often more intuitive.
-   the `REPORT()` function offered by `RTMB` is also very useful as quantities reported will be available after optimization, while the report statement is ignored during optimization.

We can now create the objective function and fit the model. This needs some explanation: At this point, `RTMB` takes the negative log-likelihood function and compiles its own version of it, including the gradient. This is what makes automatic differentiation so fast as evaluating the gradient takes similar computational resources as the original function. `MakeADFun` now also grabs whatever is saved as `dat` in the global environment and *bakes* it into the objective function. Therefore, changes to `dat` after this point will also have no result. We set `silent = TRUE` to suppress printing of the optimization process.

The optimization routine `nlminb` is very robust and conveniently allows us to provide a gradient function.

```{r modelfit}
obj = MakeADFun(mllk, par, silent = TRUE) # creating the objective function
opt = nlminb(obj$par, obj$fn, obj$gr) # optimization
```

Very nicely, `obj` is automatically updated after the optimization. We can now look at the results.

```{r objective2}
obj$fn() # function value at estimated parameter
obj$gr() # gradient at estimated parameter
```
To get our estimated parameters on their natural scale, very conveniently we don't have to do the backtransformation manually. We can just do

```{r MLe}
mod = obj$report() # runs the reporting from the negative log-likelihood once
(delta = mod$delta)
(Gamma = mod$Gamma)
(mu = mod$mu)
(sigma = mod$sigma)
(kappa = mod$kappa)
```

Note that `delta`, `Gamma` and `allprobs` are always reported by default. This is done under the hood by the `forward()` function and very useful for e.g. state decoding, because `LaMa` functions take these as inputs.

```{r decoding, fig.width = 7, fig.height = 4}
mod$states = viterbi(mod$delta, mod$Gamma, mod$allprobs)

# defining color vector
color = c("orange", "deepskyblue")

plot(elephant$step[1:200], type = "h", xlab = "time", ylab = "step length", 
     col = color[mod$states[1:200]], bty = "n")
```

As the state-dependent parameters depend on the specific model formulation, these need to be reported manually by the user specifying the negative log-likelihood. Having done this, we can use the parameters to plot the estimated state-dependent distributions.

```{r statedepdist, fig.width = 8, fig.height = 4}
oldpar = par(mfrow = c(1,2))

hist(elephant$step, prob = TRUE, breaks = 40, 
     bor = "white", main = "", xlab = "step length")
for(j in 1:2) curve(delta[j] * dgamma2(x, mu[j], sigma[j]), 
                    lwd = 2, add = T, col = color[j])
curve(delta[1]*dgamma2(x, mu[1], sigma[1]) + delta[2]*dgamma2(x, mu[2], sigma[2]), 
      lwd = 2, lty = 2, add = T)

hist(elephant$angle, prob = TRUE, breaks = 40, 
     bor = "white", main = "", xlab = "turning angle")
for(j in 1:2) curve(delta[j] * dvm(x, 0, kappa[j]), 
                    lwd = 2, add = T, col = color[j])
curve(delta[1]*dvm(x, 0, kappa[1]) + delta[2]*dvm(x, 0, kappa[2]), 
      lwd = 2, lty = 2, add = T)

par(oldpar)
```

Generalizing is also straightforward. For example, we can add time of day variation to the state process. For this we compute the trigonometric basis design matrix `Z` and add the time of day to the `dat` list. 
```{r tod}
Z = trigBasisExp(1:24, degree = 2) # building trigonometric basis desing matrix
dat$tod = elephant$tod # adding time of day to dat
```

We also need to change the parameter list `par` to include the regression parameters for the time of day. The regression parameters for the state process will typically have the form of a $N (N-1) \times p+1$ matrix, where $N$ is the number of states and $p$ is the number of regressors -- this format is also expected by `tpm_g()` which computes the array of transition matrices based on the design and parameter matrix. Very conveniently, in our parameter list we can use matrices for some parameters, making reshaping of vectors to matrices inside the likelihood function unnessesary.

```{r todpar}
par = list(logmu = log(c(0.3, 1)), 
           logsigma = log(c(0.2, 0.7)),
           logkappa = log(c(0.2, 0.7)),
           beta = matrix(c(rep(-2, 2), 
                           rep(0, 2*ncol(Z))), nrow = 2)) # 2 times 4+1 matrix
# replacing eta with regression parameters, initializing slopes at zero
```

We can now define a more general likelihood function with the main difference being the use of `tpm_g()` instead of `tpm()` and the inclusion of the time of day in the transition matrix calculation. This leads to us using `stationary_p()` instead of `stationary()` to calculate the initial distribuion and `forward_g()` instead of `forward()` to calculate the log-likelihood.

```{r mllk2}
mllk2 = function(par) {
  getAll(par, dat) # makes everything contained available without $
  Gamma = tpm_g(Z, beta, ad = TRUE)
  delta = stationary_p(Gamma, t = tod[1], ad = TRUE) # p-stationary start
  # exponentiating because all parameters strictly positive
  mu = exp(logmu)
  sigma = exp(logsigma)
  kappa = exp(logkappa)
  # report statements for later use
  REPORT(mu)
  REPORT(sigma)
  REPORT(kappa)
  # calculating all state-dependent densities
  allprobs = matrix(1, nrow = length(step), ncol = N)
  ind = which(!is.na(step) & !is.na(angle)) # only for non-NA obs.
  for(j in 1:N){
    allprobs[ind,j] = dgamma2(step[ind],mu[j],sigma[j])*dvm(angle[ind],0,kappa[j])
  }
  -forward_g(delta, Gamma[,,tod], allprobs, ad = TRUE) # simple forward algorithm
}
```

Having done this, the model fit is then essentially the same:

```{r modelfit2}
obj2 = MakeADFun(mllk2, par, silent = TRUE) # creating the objective function
opt2 = nlminb(obj2$par, obj2$fn, obj2$gr) # optimization
```

And we can look at the reported results:

```{r MLE2, fig.width = 8, fig.height = 5}
mod2 = obj2$report()
mod2$Gamma = tpm_g(Z, mod2$beta) # calculating 24 tpms
Delta = stationary_p(mod2$Gamma) # calculating all periodically stationary distributions

plot(Delta[,2], type = "b", lwd = 2, xlab = "time of day", ylab = "Pr(active)", 
     col = "deepskyblue", bty = "n", xaxt = "n")
axis(1, at = seq(0,24,by=4), labels = seq(0,24,by=4))
```

We can go one step further and model the transition probabilities as smooth functions of the time of day using cyclic P-splines. `LaMa` provides the function `make_matrices()` which creates design matrices and penalty matrices based on the R package `mgcv` when provided with a formula and data.

```{r tod2}
knots = seq(0, 24, length = 11)
modmat = make_matrices(~ s(tod, bs = "cp", k = 10), 
                       data = data.frame(tod = 1:24),
                       knots = list(tod = knots))
Z = modmat$Z # spline design matrix
S = modmat$S # penalty matrix
```

To use this, we have to change our likelihood function slightly by adding the penalization. For this we use the `penalty()` function contained in `LaMa` that computes the sum of quadratic form penalties based on the penalty matrices, the parameters to be estimated and the penalty strength parameters.

```{r mllk3}
pnll = function(par) {
  getAll(par, dat) # makes everything contained available without $
  Gamma = tpm_g(Z, cbind(beta0, betaspline), ad = TRUE)
  delta = stationary_p(Gamma, t = tod[1], ad = TRUE) # p-stationary start
  # exponentiating because all parameters strictly positive
  mu = exp(logmu)
  sigma = exp(logsigma)
  kappa = exp(logkappa)
  # report statements for later use
  REPORT(mu)
  REPORT(sigma)
  REPORT(kappa)
  # calculating all state-dependent densities
  allprobs = matrix(1, nrow = length(step), ncol = N)
  ind = which(!is.na(step) & !is.na(angle)) # only for non-NA obs.
  for(j in 1:N){
    allprobs[ind,j] = dgamma2(step[ind],mu[j],sigma[j])*dvm(angle[ind],0,kappa[j])
  }
  -forward_g(delta, Gamma[,,tod], allprobs, ad = TRUE) +
    penalty(betaspline, S, lambda)
}
```

```{r todpar2}
par = list(logmu = log(c(0.3, 1)), 
           logsigma = log(c(0.2, 0.7)),
           logkappa = log(c(0.2, 0.7)),
           beta0 = c(-2,2),
           betaspline = matrix(rep(0, 2*(ncol(Z)-1)), nrow = 2))

dat$lambda = rep(1000, 2)
dat$S = S
```

```{r pql, echo = FALSE, message=F}
mod3 = pql(pnll, par, dat, random = "betaspline")
```

```{r results pql, fig.width = 8, fig.height = 5}
mod3$Gamma = tpm_g(Z, mod3$beta) # calculating 24 tpms

tod_seq = seq(0,24, length=200)
Z_pred = pred_matrix(modmat, data.frame(tod = tod_seq))

Gamma_plot = tpm_g(Z_pred, mod3$beta)
plot(tod_seq, Gamma_plot[1,2,], type = "l", lwd = 2, ylim = c(0,1),
     xlab = "time of day", ylab = "transition probability", bty = "n")
lines(tod_seq, Gamma_plot[2,1,], lwd = 2, lty = 3)

Delta = stationary_p(mod3$Gamma) # calculating all periodically stationary distributions

plot(Delta[,2], type = "b", lwd = 2, xlab = "time of day", ylab = "Pr(active)", 
     col = "deepskyblue", bty = "n", xaxt = "n")
axis(1, at = seq(0,24,by=4), labels = seq(0,24,by=4))
```
